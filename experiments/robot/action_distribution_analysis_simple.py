#!/usr/bin/env python3
"""
Simple Action Distribution Analysis for OpenVLA

This script analyzes the distribution of actions generated by OpenVLA for a given input scene.
Focuses on the corrected implementation that should show success cases for do_sample=False.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import torch
from PIL import Image
import json
from pathlib import Path
import os
from datetime import datetime
import imageio
import tensorflow as tf

# Set up plotting
plt.style.use('default')
sns.set_palette("husl")

# Import OpenVLA modules
from experiments.robot.openvla_utils import get_vla, get_processor, crop_and_resize
from experiments.robot.robot_utils import get_model, normalize_gripper_action, invert_gripper_action
from libero.run_libero_eval import GenerateConfig
from libero.libero.envs import OffScreenRenderEnv
from libero.libero import benchmark, get_libero_path
from experiments.robot.libero.libero_utils import get_libero_dummy_action, get_libero_image, quat2axisangle


def setup_model_and_environment():
    """Setup the VLA model and LIBERO environment."""
    # Configuration
    cfg = GenerateConfig()
    cfg.model_family = "openvla"
    cfg.pretrained_checkpoint = "openvla/openvla-7b-finetuned-libero-spatial"
    cfg.task_suite_name = "libero_spatial"
    cfg.center_crop = True
    cfg.unnorm_key = cfg.task_suite_name

    # Load model and processor
    print("Loading VLA model...")
    model = get_model(cfg)
    processor = get_processor(cfg)

    print(f"Model type: {type(model)}")
    print(f"Model device: {next(model.parameters()).device}")

    # Initialize LIBERO task suite
    benchmark_dict = benchmark.get_benchmark_dict()
    task_suite = benchmark_dict[cfg.task_suite_name]()
    num_tasks_in_suite = task_suite.n_tasks

    print(f"Task suite: {cfg.task_suite_name}")
    print(f"Number of tasks available: {num_tasks_in_suite}")

    return cfg, model, processor, task_suite, num_tasks_in_suite


def load_task_and_environment(task_suite, task_id=0):
    """Load a specific task and create the environment."""
    # Select a task
    task = task_suite.get_task(task_id)
    task_description = task.language

    print(f"\nSelected Task {task_id}: {task_description}")

    # Get the BDDL file path for this task
    task_bddl_file = os.path.join(get_libero_path("bddl_files"), task.problem_folder, task.bddl_file)
    print(f"BDDL file: {task_bddl_file}")

    # Create test environment and get observation
    env_args = {
        "bddl_file_name": task_bddl_file,
        "camera_heights": 256,
        "camera_widths": 256
    }

    env = OffScreenRenderEnv(**env_args)
    obs = env.reset()

    print(f"Observation keys: {list(obs.keys())}")

    return task, task_description, env, obs


def generate_action_samples_fixed(model, processor, obs, task_label, num_samples=50, 
                                do_sample=True, temperature=1.0, unnorm_key="libero_spatial"):
    """Generate multiple action samples using proper preprocessing and direct predict_action call."""
    print(f"Generating {num_samples} action samples (do_sample={do_sample}, temp={temperature})...")
    
    # Get proper image preprocessing (same as run_libero_eval.py)
    resize_size = (256, 256)  # Same as in run_libero_eval.py
    img = get_libero_image(obs, resize_size)
    
    # Prepare observation dict (same as run_libero_eval.py)
    observation = {
        "full_image": img,
        "state": np.concatenate(
            (obs["robot0_eef_pos"], quat2axisangle(obs["robot0_eef_quat"]), obs["robot0_gripper_qpos"])
        ),
    }
    
    actions = []
    for i in range(num_samples):
        if i % 10 == 0:
            print(f"  Sample {i+1}/{num_samples}")
        
        # Use the same preprocessing as get_vla_action - use full_image from observation
        image = Image.fromarray(observation["full_image"])
        image = image.convert("RGB")
        
        # Apply center crop if needed (same as get_vla_action)
        center_crop = True
        if center_crop:
            batch_size = 1
            crop_scale = 0.9
            
            # Convert to TF Tensor and record original data type (should be tf.uint8)
            image = tf.convert_to_tensor(np.array(image))
            orig_dtype = image.dtype
            
            # Convert to data type tf.float32 and values between [0,1]
            image = tf.image.convert_image_dtype(image, tf.float32)
            
            # Crop and then resize back to original size
            image = crop_and_resize(image, crop_scale, batch_size)
            
            # Convert back to original data type
            image = tf.clip_by_value(image, 0, 1)
            image = tf.image.convert_image_dtype(image, orig_dtype, saturate=True)
            
            # Convert back to PIL Image
            image = Image.fromarray(image.numpy())
            image = image.convert("RGB")
        
        # Build VLA prompt (same as get_vla_action)
        prompt = f"In: What action should the robot take to {task_label.lower()}?\nOut:"
        
        # Process inputs (same as get_vla_action)
        inputs = processor(prompt, image).to("cuda:0", dtype=torch.bfloat16)
        
        # Get action with proper sampling parameters
        with torch.no_grad():
            print(f"      Calling predict_action with do_sample={do_sample}, temperature={temperature}")
            action = model.predict_action(
                **inputs, 
                unnorm_key=unnorm_key, 
                do_sample=do_sample,
                temperature=temperature
            )
            print(f"      Model returned action: {action}")
        
        # Apply proper action preprocessing (same as run_libero_eval.py)
        action = normalize_gripper_action(action, binarize=True)
        action = invert_gripper_action(action)
        
        # Debug: Print action values
        if i == 0:  # Only print for first sample
            print(f"    Raw action values: {action}")
            print(f"    Action shape: {action.shape}")
            print(f"    Action range: [{action.min():.3f}, {action.max():.3f}]")
            
            # Check if action values are reasonable
            if np.abs(action).max() < 0.01:
                print(f"    ⚠️  WARNING: Action values are very small! This might be the issue.")
            if np.abs(action).max() > 10:
                print(f"    ⚠️  WARNING: Action values are very large! This might cause issues.")
        
        actions.append(action)
    
    return np.array(actions)


def create_results_directory():
    """Create organized results directory structure."""
    base_dir = Path("libero/tmp/results")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = base_dir / f"action_analysis_{timestamp}"
    results_dir.mkdir(parents=True, exist_ok=True)
    (results_dir / "failures").mkdir(exist_ok=True)
    return results_dir


def evaluate_temperature_success_fixed(model, processor, obs, task_label, env,
                                     temperatures, num_samples_per_temp=20,
                                     max_steps=50, unnorm_key="libero_spatial"):
    """
    Evaluate success/failure at different temperatures using FIXED action generation.
    This should now work without the get_action() parameter error!
    """
    results_dir = create_results_directory()
    print(f"Results will be saved to: {results_dir}")
    
    results = {
        'temperatures': temperatures,
        'success_counts': {},
        'failure_counts': {},
        'success_rates': {},
        'failure_recordings': {},
        'results_dir': str(results_dir)
    }
    
    print(f"\n{'='*80}")
    print(f"FIXED TEMPERATURE-BASED SUCCESS/FAILURE ANALYSIS")
    print(f"{'='*80}")
    print(f"Task: {task_label}")
    print(f"Temperatures: {temperatures}")
    print(f"Samples per temperature: {num_samples_per_temp}")
    print(f"Max steps per evaluation: {max_steps}")
    print(f"Using FIXED action generation (no get_action() parameter error)")
    
    for temp_idx, temperature in enumerate(temperatures):
        print(f"\n{'-'*60}")
        print(f"EVALUATING TEMPERATURE: {temperature}")
        print(f"{'-'*60}")
        
        # Generate actions at this temperature using FIXED function
        print(f"Generating {num_samples_per_temp} actions at temperature {temperature}...")
        if temperature == 0:
            actions_at_temp = generate_action_samples_fixed(
                model, processor, obs, task_label, 
                num_samples=num_samples_per_temp, 
                do_sample=False,  # Deterministic for temperature 0
                unnorm_key=unnorm_key
            )
        else:
            actions_at_temp = generate_action_samples_fixed(
                model, processor, obs, task_label, 
                num_samples=num_samples_per_temp, 
                do_sample=True, 
                temperature=temperature,
                unnorm_key=unnorm_key
            )
        
        # Evaluate each action
        successes = []
        failures = []
        failure_recordings = []
        
        print(f"Evaluating {len(actions_at_temp)} actions...")
        
        for action_idx, action in enumerate(actions_at_temp):
            print(f"  Evaluating action {action_idx+1}/{len(actions_at_temp)}...")
            
            # Reset environment
            env.reset()
            
            # Wait for objects to settle using LIBERO evaluation logic
            num_steps_wait = 5  # Same as cfg.num_steps_wait in the evaluation script
            for t in range(num_steps_wait):
                dummy_action = get_libero_dummy_action("openvla")
                obs_step, reward, done, info = env.step(dummy_action)
            
            # Record the simulation
            recording_images = []
            success = False
            steps_taken = 0
            
            # Capture initial state
            recording_images.append(obs_step["agentview_image"][::-1])
            
            for step in range(max_steps):
                # Execute action
                obs_step, reward, done, info = env.step(action.tolist())
                steps_taken += 1
                
                # Debug: Print step info
                if action_idx == 0 and step < 5:  # Only for first action, first 5 steps
                    print(f"      Step {step}: reward={reward:.3f}, done={done}, info={info}")
                
                # Capture frame
                recording_images.append(obs_step["agentview_image"][::-1])
                
                if done:
                    success = True
                    print(f"      ✅ Task completed! Reward: {reward}")
                    break
            
            if success:
                successes.append({
                    'action': action,
                    'steps_taken': steps_taken,
                    'temperature': temperature,
                    'action_idx': action_idx
                })
                print(f"    ✅ SUCCESS in {steps_taken} steps")
            else:
                failures.append({
                    'action': action,
                    'steps_taken': steps_taken,
                    'temperature': temperature,
                    'action_idx': action_idx
                })
                print(f"    ❌ FAILURE after {steps_taken} steps")
                
                # Save failure recording
                failure_filename = f"failure_temp_{temperature}_action_{action_idx:03d}.mp4"
                failure_path = results_dir / "failures" / failure_filename
                
                # Save video recording
                with imageio.get_writer(str(failure_path), fps=10) as writer:
                    for img in recording_images:
                        writer.append_data(img)
                
                failure_recordings.append({
                    'filename': failure_filename,
                    'path': str(failure_path),
                    'action': action.tolist(),
                    'steps_taken': steps_taken,
                    'temperature': temperature
                })
        
        # Store results for this temperature
        success_count = len(successes)
        failure_count = len(failures)
        success_rate = success_count / num_samples_per_temp
        
        results['success_counts'][temperature] = success_count
        results['failure_counts'][temperature] = failure_count
        results['success_rates'][temperature] = success_rate
        results['failure_recordings'][temperature] = failure_recordings
        
        print(f"\n📊 TEMPERATURE {temperature} RESULTS:")
        print(f"  Successes: {success_count}/{num_samples_per_temp} ({success_rate:.1%})")
        print(f"  Failures:  {failure_count}/{num_samples_per_temp} ({1-success_rate:.1%})")
        print(f"  Failure recordings saved: {len(failure_recordings)}")
    
    return results


def analyze_temperature_results(results):
    """Analyze and visualize the temperature-based results."""
    
    print(f"\n{'='*80}")
    print(f"TEMPERATURE ANALYSIS SUMMARY")
    print(f"{'='*80}")

    # Create summary table
    print(f"{'Temperature':<12} {'Successes':<10} {'Failures':<10} {'Success Rate':<12} {'Failure Videos':<15}")
    print("-" * 80)

    for temp in results['temperatures']:
        successes = results['success_counts'][temp]
        failures = results['failure_counts'][temp]
        success_rate = results['success_rates'][temp]
        failure_videos = len(results['failure_recordings'][temp])

        print(f"{temp:<12} {successes:<10} {failures:<10} {success_rate:<12.1%} {failure_videos:<15}")

    # Create visualizations
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Temperature-based Success/Failure Analysis', fontsize=16)

    # Plot 1: Success rate vs temperature
    ax1 = axes[0, 0]
    temps = results['temperatures']
    success_rates = [results['success_rates'][t] for t in temps]
    ax1.plot(temps, success_rates, 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('Temperature')
    ax1.set_ylabel('Success Rate')
    ax1.set_title('Success Rate vs Temperature')
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)

    # Add value labels on points
    for i, (temp, rate) in enumerate(zip(temps, success_rates)):
        ax1.annotate(f'{rate:.1%}', (temp, rate), textcoords="offset points",
                    xytext=(0,10), ha='center')

    # Plot 2: Success count vs temperature
    ax2 = axes[0, 1]
    success_counts = [results['success_counts'][t] for t in temps]
    failure_counts = [results['failure_counts'][t] for t in temps]

    x = np.arange(len(temps))
    width = 0.35

    ax2.bar(x - width/2, success_counts, width, label='Successes', color='green', alpha=0.7)
    ax2.bar(x + width/2, failure_counts, width, label='Failures', color='red', alpha=0.7)

    ax2.set_xlabel('Temperature')
    ax2.set_ylabel('Count')
    ax2.set_title('Success vs Failure Counts')
    ax2.set_xticks(x)
    ax2.set_xticklabels(temps)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Plot 3: Failure video count vs temperature
    ax3 = axes[1, 0]
    failure_video_counts = [len(results['failure_recordings'][t]) for t in temps]
    ax3.bar(temps, failure_video_counts, color='red', alpha=0.7)
    ax3.set_xlabel('Temperature')
    ax3.set_ylabel('Number of Failure Videos')
    ax3.set_title('Failure Recordings Saved')
    ax3.grid(True, alpha=0.3)

    # Plot 4: Success rate distribution
    ax4 = axes[1, 1]
    ax4.hist(success_rates, bins=len(temps), alpha=0.7, color='blue', edgecolor='black')
    ax4.set_xlabel('Success Rate')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Success Rate Distribution')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Find optimal temperature
    best_temp_idx = np.argmax(success_rates)
    best_temp = temps[best_temp_idx]
    best_rate = success_rates[best_temp_idx]

    print(f"\n🎯 OPTIMAL TEMPERATURE: {best_temp} (Success Rate: {best_rate:.1%})")

    # Print failure recording summary
    total_failures = sum(results['failure_counts'].values())
    total_recordings = sum(len(results['failure_recordings'][t]) for t in temps)

    print(f"\n📹 FAILURE RECORDINGS:")
    print(f"  Total failures: {total_failures}")
    print(f"  Recordings saved: {total_recordings}")
    print(f"  Results directory: {results['results_dir']}")

    return {
        'best_temperature': best_temp,
        'best_success_rate': best_rate,
        'total_failures': total_failures,
        'total_recordings': total_recordings
    }


def main():
    """Main function to run the action distribution analysis."""
    print("🚀 Starting Action Distribution Analysis for OpenVLA")
    print("=" * 60)
    
    # Setup
    cfg, model, processor, task_suite, num_tasks_in_suite = setup_model_and_environment()
    
    # Load task and environment - try a simpler task
    task, task_description, env, obs = load_task_and_environment(task_suite, task_id=8)  # Task 8: "pick up the black bowl next to the plate and place it on the plate"
    task_label = task_description
    
    # Show available tasks
    print("\nAvailable LIBERO Spatial Tasks:")
    print("=" * 50)
    for i in range(min(10, num_tasks_in_suite)):  # Show first 10 tasks
        task = task_suite.get_task(i)
        print(f"Task {i:2d}: {task.language}")
    print(f"\n... and {max(0, num_tasks_in_suite - 10)} more tasks")
    
    # Action dimension names
    action_names = [
        'x_delta', 'y_delta', 'z_delta', 
        'roll_delta', 'pitch_delta', 'yaw_delta', 
        'gripper'
    ]
    action_dim = len(action_names)
    print(f"Action dimensions: {action_dim}")
    
    # Test the FIXED implementation
    print("\n🧪 TESTING FIXED IMPLEMENTATION")
    print("=" * 50)

    # Test with a small number of samples first
    test_temperatures = [0, 1.0]  # Test deterministic (0) and sampled (1.0)
    test_samples = 3  # Small number for quick test

    print(f"Testing temperatures: {test_temperatures}")
    print(f"Samples per temperature: {test_samples}")
    print(f"Task: {task_label}")
    
    # Debug: Check if we can see the scene
    print(f"\n🔍 DEBUGGING INFO:")
    print(f"  Environment type: {type(env)}")
    print(f"  Observation keys: {list(obs.keys())}")
    print(f"  Robot position: {obs['robot0_eef_pos']}")
    print(f"  Robot gripper: {obs['robot0_gripper_qpos']}")
    
    # Try a simple action to see if the environment responds
    print(f"  Testing environment with dummy action...")
    dummy_action = get_libero_dummy_action("openvla")
    test_obs, test_reward, test_done, test_info = env.step(dummy_action)
    print(f"  Dummy action result: reward={test_reward}, done={test_done}")
    env.reset()  # Reset after test

    # Test with a single action first to debug
    print(f"\n🔬 SINGLE ACTION TEST:")
    print(f"  Generating 1 action with do_sample=False...")
    
    # Test single action generation
    single_action = generate_action_samples_fixed(
        model, processor, obs, task_label, 
        num_samples=1, 
        do_sample=False,  # Deterministic
        unnorm_key=cfg.unnorm_key
    )[0]
    
    print(f"  Generated action: {single_action}")
    print(f"  Action magnitude: {np.linalg.norm(single_action):.3f}")
    
    # Test this single action in the environment
    print(f"  Testing single action in environment...")
    env.reset()
    
    # Wait for objects to settle
    for t in range(5):
        dummy_action = get_libero_dummy_action("openvla")
        obs_step, reward, done, info = env.step(dummy_action)
    
    # Execute the generated action
    obs_step, reward, done, info = env.step(single_action.tolist())
    print(f"  Single action result: reward={reward}, done={done}, info={info}")
    
    # Run the FIXED evaluation
    fixed_results = evaluate_temperature_success_fixed(
        model, processor, obs, task_label, env,
        temperatures=test_temperatures,
        num_samples_per_temp=test_samples,
        max_steps=220,  # Increased steps to see if task needs more time
        unnorm_key=cfg.unnorm_key
    )

    print("\n🎯 FIXED RESULTS SUMMARY:")
    print("=" * 50)
    for temp in test_temperatures:
        successes = fixed_results['success_counts'][temp]
        failures = fixed_results['failure_counts'][temp]
        success_rate = fixed_results['success_rates'][temp]
        print(f"Temperature {temp}: {successes} successes, {failures} failures ({success_rate:.1%} success rate)")

    # Check if we now see success cases for do_sample=False (temperature 0)
    temp_0_successes = fixed_results['success_counts'][0]
    if temp_0_successes > 0:
        print(f"\n🎉 SUCCESS! Found {temp_0_successes} success cases for do_sample=False (temperature 0)")
        print("The FIXED implementation is working!")
    else:
        print(f"\n⚠️  Still no success cases for do_sample=False (temperature 0)")
        print("May need further investigation...")

    print(f"\n📊 COMPARISON:")
    print(f"  Temperature 0 (do_sample=False): {fixed_results['success_rates'][0]:.1%} success rate")
    print(f"  Temperature 1.0 (do_sample=True): {fixed_results['success_rates'][1.0]:.1%} success rate")
    
    # Run full analysis if test was successful
    if temp_0_successes > 0:
        print("\n🎯 Running full temperature analysis...")
        
        # Define temperatures to test
        temperatures = [0.1, 0.5, 1.0, 1.5, 2.0, 3.0]
        num_samples_per_temp = 15  # Adjust this for faster/slower analysis
        max_steps = 30  # Adjust this for shorter/longer simulations

        # Run the analysis
        temp_results = evaluate_temperature_success_fixed(
            model, processor, obs, task_label, env,
            temperatures=temperatures,
            num_samples_per_temp=num_samples_per_temp,
            max_steps=max_steps,
            unnorm_key=cfg.unnorm_key
        )
        
        # Analyze and visualize results
        analysis_summary = analyze_temperature_results(temp_results)
        
        print(f"\n🎯 EXPERIMENT COMPLETE!")
        print(f"  • Tested {len(temp_results['temperatures'])} temperatures")
        print(f"  • Generated {sum(temp_results['success_counts'].values()) + sum(temp_results['failure_counts'].values())} total actions")
        print(f"  • Saved {analysis_summary['total_recordings']} failure recordings")
        print(f"  • Best temperature: {analysis_summary['best_temperature']} ({analysis_summary['best_success_rate']:.1%} success rate)")
    
    return fixed_results


if __name__ == "__main__":
    results = main()
